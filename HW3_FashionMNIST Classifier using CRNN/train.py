# -*- coding: utf-8 -*-
"""FashionMNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hk16PPRAHeWSGJLg7azSkksCUnAVr8qG
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

"""1. Define Hyper-parameters and pre-set device on cuda"""

# Device Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"device: {device}")

# Adjust Dataset classes and channels
num_classes = 10  # FashionMNIST의 클래스는 10개
in_channel = 1    # 흑백 이미지이기 때문에 입력 채널은 1

# Hyper-parameters
batch_size = 50
max_pool_kernel = 2
learning_rate = 0.0005
num_epochs = 15

"""2. Load Data & Split into Validation and Train"""

train_data = torchvision.datasets.FashionMNIST(root='./datasets',
                                               train=True,
                                               transform=transforms.ToTensor(),
                                               download=True)

test_data = torchvision.datasets.FashionMNIST(root='./datasets',
                                              train=False,
                                              transform=transforms.ToTensor(),
                                              download=True)

len(train_data), len(test_data)

## train data의 10%를 validation data로 분할한다
train_size = int(0.9 * len(train_data))
val_size = len(train_data) - train_size

train_data, val_data = torch.utils.data.random_split(train_data, [train_size, val_size])

len(train_data), len(val_data), len(test_data)

"""10%로 validation을 만들고 난 후 train이 5만 4천개, validation이 6천개인 것을 확인할 수 있다.

3. Image for each class
"""

# FashionMNIST
plt.figure(figsize=(10,5))
cnt = 0
classList = [] # 클래스의 이름을 저장하여 현재까지 나온 클래스의 개수를 세기 위한 리스트

while True:
  if len(classList) == 10: break                                  # 10개의 클래스가 모두 출력되었으면 종료
  cnt += 1
  if test_data.classes[test_data[cnt][1]] not in classList:       # 아직 나오지 않은 클래스라면
    classList.append(test_data.classes[test_data[cnt][1]])        # 리스트에 추가하고
    plt.subplot(2, 5, len(classList))                             # 위치 지정하고
    plt.title(test_data.classes[test_data[cnt][1]])               # 타이틀로 클래스 적고
    plt.imshow(test_data.data[cnt].reshape(28,28), cmap='gray')   # 이미지 출력

"""4. Define Dataloader"""

train_loader = torch.utils.data.DataLoader(dataset=train_data,
                                           batch_size=batch_size,
                                           shuffle=True)

val_loader = torch.utils.data.DataLoader(dataset=val_data,
                                         batch_size=batch_size,
                                         shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_data,
                                          batch_size=batch_size,
                                          shuffle=False)

image, label = next(iter(test_loader))
print(image.size()) # [Batch, Channel, Height, Width]
                    # batch size로 가져온 데이터 개수, 입력 채널(1), 28 x 28(FashionMNIST의 이미지 형식)
                    # FashionMNIST는 흑백이기 때문에 입력 채널이 1이다

"""5. Define Model"""

sequence_length = 7*8   # 입력으로 주는 sequence를 몇으로 줄 것이냐
input_size = 7*8        # input data의 차원
hidden_size = 128       # hidden state의 차원
num_layers = 5          # RNN의 은닉층 레이어 개수

class CRNN(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, num_classes):
    super(CRNN, self).__init__()

    # 1st Layer - Convolution
    self.layer1 = nn.Sequential(  # 28x28x1 --> padding==2 --> 32x32x1 --> filter 5x5 --> 28x28x32
        nn.Conv2d(in_channels=in_channel, out_channels=32, kernel_size=5, stride=1, padding=2), # 5x5 kernel
        nn.BatchNorm2d(num_features= 32),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 28x28x32 --> 2x2 max_pooling --> 14x14x32
    )
    # 2nd Layer - Convolution
    self.layer2 = nn.Sequential( # 14x14x32 --> padding==2 --> 18x18x32 --> filter 5x5 --> 14x14x64
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),
        nn.BatchNorm2d(num_features=64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 14x14x64 --> 2x2 max_pooling --> 7x7x64
    )

    # 2개의 Covolution Layer를 지나고 RNN에 들어가기 전의 이미지 구조는 7x7x64채널이다
    # 이것을 sequence length와 input size로 분할하기 위해 각각 7x8로 설정하였다
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.4)
    # "batch_first=True" 옵션으로 x -> (batch_size, seq, input_size) 로 dimension이 정해진다

    self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)

  def forward(self, x): # 설정한 변수들로 순전파
    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # 0으로 이루어진 텐서로 초기화
                                                       # (hidden layer개수, batch_size, hidden state의 차원)
    x = self.layer1(x)
    x = self.layer2(x)

    # RNN에 들어갈 때 batch_size, seq_length, input_size로 맞춰줘야 한다
    x = x.reshape(x.size(0), -1, input_size) # x.size(0) = batch_size, input_size를 넣어주고 seq_length를 알아서 계산

    out, _  = self.rnn(x, (h0)) # out -> (batch_size, seq_length, hidden_size)

    # 필요한 것은 마지막 시퀀스의 출력 뿐이므로, out -> (batch_size, -1, hidden_size)로 설정해준다
    out = self.fc(out[:,-1,:]) # 마지막 out만 가져와서 fully connected에 넣는다

    return out

model = CRNN(input_size, hidden_size, num_layers, num_classes).to(device)

"""6. Set Loss & Optimizer"""

criterion = nn.CrossEntropyLoss()                                   # 분류 문제이기에 CrossEntropy로 loss를 계산
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # optimizer는 adam으로 설정
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)

"""7. Train with Validation"""

total_step = len(train_loader)

# Train Loop
tr_loss = []
tr_acc = []
v_loss = []
v_acc = []

best_valid_loss = torch.inf
best_epoch = 0

for epoch in range(num_epochs):
  ####### Train #######
  model.train()
  train_acc = 0
  train_loss = 0

  for i, (image, label) in enumerate(train_loader):
    image = image.to(device) # 처음에 Convolution layer에 들어갈때는 이런 형태로 들어가니깐
    #image = image.reshape(-1, sequence_length, input_size).to(device) # 이미지를 reshape
    label = label.to(device)

    # Forward
    output = model(image)
    loss = criterion(output, label)

    _, pred = torch.max(output.data, 1)
    train_acc += (pred == label).sum().item()

    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    train_loss += loss.item()

  scheduler.step()
  ####### Validation #######
  model.eval()
  val_acc = 0
  val_loss = 0

  with torch.no_grad():
    for i, (image, label) in enumerate(val_loader):
      image = image.to(device) # 처음에 Convolution layer에 들어갈때는 이런 형태로 들어가니깐
      #image = image.reshape(-1, sequence_length, input_size).to(device) # 이미지를 reshape
      label = label.to(device)

      # forward pass
      out = model(image)
      _, pred = torch.max(out.data, 1)
      val_acc += (pred == label).sum().item()

      # loss
      loss = criterion(out, label)
      val_loss += loss.item()

  # save model if validation loss decrease
  if val_loss / len(val_data) <= best_valid_loss:
      best_valid_loss = val_loss / len(val_loader)
      best_epoch = epoch
      torch.save(model.state_dict(), "CRNN_epoch.pth") # 에폭이 끝날 때마다 model을 저장해놓는데 val_loss가 가장 작은 시점의 model이 저장되어 있을 것이다
                                                                        # 다 저장하고 loss graph 보고 골라서 사용할 수도 있다
                                                                        # val이 좋아지다가 떨어지는 지점이면 overfitting이라 생각하게 되는데 모델이 저장되어 있지 않으면 쓸모가 없어지기 때문에
                                                                        # 이러한 코드를 작성한 것이라 볼 수 있다

  # print epoch loss & accuracy
  print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss / len(train_loader):.3f} Train Acc: {(train_acc / len(train_data) * 100):.3f}% | Val Loss: {val_loss / len(val_loader):.3f} Val Acc: {(val_acc / len(val_data) * 100):.3f}%')
  tr_loss.append(train_loss / len(train_loader))
  tr_acc.append(train_acc / len(train_data) * 100)
  v_loss.append(val_loss / len(val_loader))
  v_acc.append(val_acc / len(val_data) * 100)

"""8. Visualization & Summary"""

plt.plot(range(num_epochs), tr_loss, label="Train Loss")
plt.plot(range(num_epochs), v_loss, label="Val Loss")
plt.legend()
plt.title("Loss - epoch")
plt.show()

plt.plot(range(num_epochs), tr_acc, label="Train Acc")
plt.plot(range(num_epochs), v_acc, label="Val Acc")
plt.legend()
plt.title("Accuracy - epoch")
plt.show()

from torchsummary import summary
summary(model, (1, 28, 28))