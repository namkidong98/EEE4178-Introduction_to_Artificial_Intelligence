# -*- coding: utf-8 -*-
"""CRNN_test

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ms1mT0plrxRZfEhx4emYXQzoCrtMI1np
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms

import matplotlib
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

# Device Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"device: {device}")

# Adjust Dataset classes and channels
num_classes = 10  # FashionMNIST의 클래스는 10개
in_channel = 1    # 흑백 이미지이기 때문에 입력 채널은 1

# Hyper-parameters
batch_size = 50
max_pool_kernel = 2
num_epochs = 15

test_data = torchvision.datasets.FashionMNIST(root='./datasets',
                                              train=False,
                                              transform=transforms.ToTensor(),
                                              download=True)

test_loader = torch.utils.data.DataLoader(dataset=test_data,
                                          batch_size=batch_size,
                                          shuffle=False)

sequence_length = 7*8   # 입력으로 주는 sequence를 몇으로 줄 것이냐
input_size = 7*8        # input data의 차원
hidden_size = 128       # hidden state의 차원
num_layers = 5          # RNN의 은닉층 레이어 개수

class CRNN(nn.Module):
  def __init__(self, input_size, hidden_size, num_layers, num_classes):
    super(CRNN, self).__init__()

    # 1st Layer - Convolution
    self.layer1 = nn.Sequential(  # 28x28x1 --> padding==2 --> 32x32x1 --> filter 5x5 --> 28x28x32
        nn.Conv2d(in_channels=in_channel, out_channels=32, kernel_size=5, stride=1, padding=2), # 5x5 kernel
        nn.BatchNorm2d(num_features= 32),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 28x28x32 --> 2x2 max_pooling --> 14x14x32
    )
    # 2nd Layer - Convolution
    self.layer2 = nn.Sequential( # 14x14x32 --> padding==2 --> 18x18x32 --> filter 5x5 --> 14x14x64
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),
        nn.BatchNorm2d(num_features=64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 14x14x64 --> 2x2 max_pooling --> 7x7x64
    )

    # 2개의 Covolution Layer를 지나고 RNN에 들어가기 전의 이미지 구조는 7x7x64채널이다
    # 이것을 sequence length와 input size로 분할하기 위해 각각 7x8로 설정하였다
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=0.4)
    # "batch_first=True" 옵션으로 x -> (batch_size, seq, input_size) 로 dimension이 정해진다

    self.fc = nn.Linear(in_features=hidden_size, out_features=num_classes)

  def forward(self, x): # 설정한 변수들로 순전파
    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # 0으로 이루어진 텐서로 초기화
                                                       # (hidden layer개수, batch_size, hidden state의 차원)
    x = self.layer1(x)
    x = self.layer2(x)

    # RNN에 들어갈 때 batch_size, seq_length, input_size로 맞춰줘야 한다
    x = x.reshape(x.size(0), -1, input_size) # x.size(0) = batch_size, input_size를 넣어주고 seq_length를 알아서 계산

    out, _  = self.rnn(x, (h0)) # out -> (batch_size, seq_length, hidden_size)

    # 필요한 것은 마지막 시퀀스의 출력 뿐이므로, out -> (batch_size, -1, hidden_size)로 설정해준다
    out = self.fc(out[:,-1,:]) # 마지막 out만 가져와서 fully connected에 넣는다

    return out

test_model = CRNN(input_size, hidden_size, num_layers, num_classes).to(device)

# 모델에 state_dict 로드
test_model.load_state_dict(torch.load('crnn_20180032.pth'))

# 모델을 평가 모드로 설정
test_model.eval()

with torch.no_grad():
  correct = 0

  for image, label in test_loader:
    image = image.to(device)
    label = label.to(device)
    output = test_model(image)
    _, pred = torch.max(output.data, 1)
    correct += (pred == label).sum().item()

  print('Test Accuracy of CRNN model on the {} test images: {}%'.format(len(test_data), 100 * correct / len(test_data)))