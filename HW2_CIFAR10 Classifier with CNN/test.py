# -*- coding: utf-8 -*-
"""CNN_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yr31Y0lswzvPDxYdpUwMcsULmmhi5qYk
"""

import torch
import torchvision
import torchvision.transforms as transforms

# 추가한 라이브러리
import matplotlib.pyplot as plt # 시각화
import numpy as np
import torch.nn as nn
import torch.nn.functional as F

# Device Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 최종 클래스와 입력 채널을 설정
num_classes = 10 # CIFAR10은 10개의 클래스로 설정
in_channel = 3   # CIFAR10은 컬러 이미지이기 때문에(RGB) 3개 채널로 입력

# 하이퍼 파라미터 설정
batch_size = 50
max_pool_kernel = 2 # "Max-pooling layer"의 kernel size는 2x2로 구성"
learning_rate = 0.001
num_epochs = 50

test_data = torchvision.datasets.CIFAR10(root='./datasets',
                                       train=False,         # 테스트 데이터를 받겠다
                                       transform=transforms.ToTensor(), # tensor로 변환
                                       download=True)

test_loader = torch.utils.data.DataLoader(dataset=test_data,
                                          batch_size=batch_size,
                                          shuffle=False)         # test의 경우에는 shuffle이 필요 없음

class ConvNetStep(nn.Module):
  def __init__(self, num_classes=10):
    super(ConvNetStep, self).__init__()

    # 1st Layer
    self.layer1 = nn.Sequential(  # 32x32 in_channel==3 --> padding==2 --> 36x36, channel=3 --> cnn 5x5 --> 32x32 out_channel=32
        nn.Conv2d(in_channels=in_channel, out_channels=32, kernel_size=5, stride=1, padding=2),
        nn.BatchNorm2d(num_features=32), # convolution의 output channel 그대로
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 32x32, channel=32 --> 2x2 max_pooling --> 16x16, channel=32
    )
    # 2nd Layer
    self.layer2 = nn.Sequential( # 16x16, in_channel=32 --> padding==2 --> 20x20, channel=32 --> cnn 5x5 --> 16x16, out_channel=64
        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),
        nn.BatchNorm2d(num_features=64), # out_channel 그대로
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 16x16, channel=64 --> 2x2 max_pooling --> 8x8, channel=64
    )
    # 3rd layer
    self.layer3 = nn.Sequential( # 8x8, in_channel=64 --> padding==2 --> 12x12, channel=128 --> cnn 5x5 --> 8x8, out_channel=128
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),
        nn.BatchNorm2d(num_features=128), # out_channel 그대로
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=max_pool_kernel) # 8x8, channel=128 --> 2x2 max_pooling --> 4x4, channel=128
    )

    # 4th Layer(1st Fully_connected)
    self.fc1 = nn.Linear(in_features=4 * 4 * 128, out_features= 100)   # 2048 --> 100
    self.dropout1 = nn.Dropout(p=0.5)

    # 5th Layer(2nd Fully_connected)
    self.fc2 = nn.Linear(in_features=100, out_features=50)            #  100 -->  50
    self.dropout2 = nn.Dropout(p=0.5)

    # Output Layer(3rd Fully_connected)
    self.fc3 = nn.Linear(in_features=50, out_features=num_classes)    #   50 -->  10

  def forward(self, x):    # 실제 학습 시에는 이 함수만 사용
    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)

    x = F.relu(x)
    x = x.reshape(x.size(0),-1) # fully_connected에 넣을 수 있도록 flatten

    x = F.relu(self.fc1(x))
    x = self.dropout1(x)  # 드롭아웃 적용
    x = F.relu(self.fc2(x))
    x = self.dropout2(x)  # 드롭아웃 적용
    x = self.fc3(x)

    return x

test_model = ConvNetStep().to(device)

test_model.load_state_dict(torch.load('model.pth'))

test_model.eval() # Set model as evaluation mode

with torch.no_grad(): # auto_grad off
  correct = 0
  total = 0
  for images, labels in test_loader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = test_model(images)
    _, predicted = torch.max(outputs.data, 1)

    total += labels.size(0)
    correct += (predicted == labels).sum().item()

  print('Accuracy of the network on the {} test images {}%'.format(len(test_loader)*batch_size, 100*correct/total))
